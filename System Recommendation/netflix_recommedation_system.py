# -*- coding: utf-8 -*-
"""Netflix Recommedation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ijZWat1s_nu1UC3u8h2JGjxbDboRE83

# Import Data
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from google.colab import files
files.upload()  # Upload kaggle.json

import os
import zipfile

# Buat direktori .kaggle dan pindahkan credential
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""# Preparation Data 1"""

!kaggle datasets download -d narayan63/netflix-popular-movies-dataset

# Ekstrak data
!unzip netflix-popular-movies-dataset.zip

df = pd.read_csv('n_movies.csv')
df

"""# Preprocessing Data 1

Meninjau data yang digunakan
"""

df.info()

"""Hanya kolom "rating" yang terhitung sebagai kolom numerik. Tipe data akan diperbaiki nanti."""

df.shape

"""Manage null and duplicates data"""

df.isnull().sum()

"""karena kolom year, certificate, dan duration tidak relevan dalam pembuatan sistem rekomendasi dan memiliki null values yg tinggi, maka akan didrop kolom tersebut alih-alih drop null values."""

# drop columns year, certificate, and duration

df.drop(columns=['year', 'certificate', 'duration'], inplace=True)
df.isnull().sum()

# drop null values
df.dropna(inplace=True)

"""mencari tahu jumlah duplicate data"""

df.duplicated().sum()

"""Change Data Type"""

# mengubah tipe data pada kolom 'votes' menjadi int
df['votes'] = df['votes'].astype(str).str.replace(',', '', regex=False).astype(int)

"""Ekstraksi fitur dengan TF-IDF (Term Frequency-Inverse Document Frequency)"""

# fillna NaN
df = df.reset_index(drop=True)
df['description'] = df['description'].fillna('')

# Menghapus semua stop word dalam bahasa inggris seperti 'a', 'the', dll.
tfidf = TfidfVectorizer(stop_words='english')

# membuat matriks TF-IDF dengan fit transform data
tfidf_matrix = tfidf.fit_transform(df['description'])

tfidf_matrix.shape

"""Parsing kolom `stars` yang berupa string list dikonversi menggunakan `literal_eval`"""

from ast import literal_eval

# Ubah string yang terlihat seperti list menjadi list yang terbaca pyhton
df['stars'] = df['stars'].apply(literal_eval)

"""# Modelling: Content-Based Recommender

Pada tahap ini, dilakukan modelling content-based filtering

## Plot Description-Based Recommender

Pada filtering ini, akan dibuat recommender yang mengambil kesimpulan berdasarkan kesamaan plot suatu film. Deskripsi plot diambil dari fitur 'description' dari dataset.

Pada tahap ini, kita akan membuat sistem rekomendasi dengan cara menghitung similarity menggunakan cosine similarity.
"""

# hitung matriks cosine similarity
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# membuat reverse map dari index dan judul film
indices = pd.Series(df.index, index=df['title']).drop_duplicates()

# Function that takes in movie title as input and outputs most similar movies
def get_recommendations(title, cosine_sim=cosine_sim):
    title = title.lower()
    matched_indices = indices[indices.index.str.lower() == title]
    if matched_indices.empty:
      return f"Judul '{title}' tidak ditemukan."

    idx = matched_indices.values[0]  # atau .item()

    # mengambil similarity scores dari seluruh film
    sim_scores = list(enumerate(cosine_sim[idx]))

    # menyortir film berdasarkan similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # mengambil skor dari top 10 most similar movies
    sim_scores = sim_scores[1:11]

    # mengambil index film
    movie_indices = [i[0] for i in sim_scores]

    # mengembalikan list top 10 most similar movies
    return df['title'].iloc[movie_indices]

print(cosine_sim.shape)  # Harus (N, N) dengan N = jumlah baris df
print(df.shape)          # Harus sama dengan N

"""## Cast and Genres Based Recommender

Pada tahap ini kita akan membuat sistem rekomendasi film berdasarkan kesamaan cast dan genre.
"""

# memperbaiki entri pada kolom 'stars' agar mudah dibaca
def clean_stars(entry):
    # Jika isinya list, gabungkan, buang spasi dan koma berlebih
    if isinstance(entry, list):
        return ', '.join([s.strip().rstrip(',') for s in entry])
    return entry  # biarkan kalau bukan list

# Terapkan ke dataframe
df['stars'] = df['stars'].apply(clean_stars)
df.head()

def cast_genre_recommender(df):
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    df = df.copy()

    # Asumsikan kolom 'stars' sudah berupa string bersih hasil preprocessing
    df['stars'] = df['stars'].fillna('').astype(str)
    df['genre'] = df['genre'].fillna('').astype(str)

    # Gabungkan fitur stars dan genre menjadi satu kolom 'soup'
    df['soup'] = df['stars'] + ' ' + df['genre']

    # Vektorisasi dan hitung kemiripan cosine
    count = CountVectorizer(stop_words='english')
    count_matrix = count.fit_transform(df['soup'])
    cosine_sim = cosine_similarity(count_matrix, count_matrix)

    # Buat index judul
    df = df.reset_index(drop=True)
    indices = pd.Series(df.index, index=df['title'].str.lower()).drop_duplicates()

    # Fungsi untuk rekomendasi
    def get_recommendations(title, top_n=10):
        title = title.lower()
        if title not in indices:
            return f"Judul '{title}' tidak ditemukan."

        idx = indices[title]
        if isinstance(idx, pd.Series):
            idx = idx.iloc[0]

        sim_scores = list(enumerate(cosine_sim[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
        movie_indices = [i[0] for i in sim_scores]
        return df.loc[movie_indices, ['title', 'genre', 'stars']].reset_index(drop=True)

    return get_recommendations

"""contoh list rekomendasi top 10"""

recommender = cast_genre_recommender(df)
recommender("The Crown")

"""# Modelling 2: Hybrid Recommender

Pada tahap ini, dilakukan modelling dengan hybrid filtering, caranya dengan menggabungkan content-based filtering dengan bobot TF-IDF.
"""

def hybrid_recommender(df, weight_cast_genre=0.5, weight_plot=0.5):
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    df = df.copy()

    # Pastikan kolom string dan tidak null
    df['stars'] = df['stars'].fillna('').astype(str)
    df['genre'] = df['genre'].fillna('').astype(str)
    df['description'] = df['description'].fillna('').astype(str)

    # Gabungkan fitur metadata
    df['soup'] = df['stars'] + ' ' + df['genre']

    # Vectorizer untuk soup dan description
    count = CountVectorizer(stop_words='english')
    count_matrix = count.fit_transform(df['soup'])

    tfidf = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf.fit_transform(df['description'])

    # Hitung cosine similarity
    cosine_sim_soup = cosine_similarity(count_matrix, count_matrix)
    cosine_sim_plot = cosine_similarity(tfidf_matrix, tfidf_matrix)

    # Gabungkan kedua similarity
    cosine_sim_hybrid = (weight_cast_genre * cosine_sim_soup) + (weight_plot * cosine_sim_plot)

    # Mapping judul ke index
    df = df.reset_index(drop=True)
    indices = pd.Series(df.index, index=df['title'].str.lower()).drop_duplicates()

    # Fungsi rekomendasi
    def get_hybrid_recommendations(title, top_n=10):
        title = title.lower()
        if title not in indices:
            return f"Judul '{title}' tidak ditemukan."

        idx = indices[title]
        if isinstance(idx, pd.Series):
            idx = idx.iloc[0]

        sim_scores = list(enumerate(cosine_sim_hybrid[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
        movie_indices = [i[0] for i in sim_scores]
        return df.loc[movie_indices, ['title', 'genre', 'stars']].reset_index(drop=True)

    return get_hybrid_recommendations

"""contoh list rekomendasi film top 10"""

get_hybrid = hybrid_recommender(df)
get_hybrid("The Crown", top_n=10)

"""# Preparation & Preprocessing Data 2

Setalah dilakukan modelling, selanjutnya dilakukan evaluasi model dengan cara mengekstrak dataset baru berisi rating user. Dataset diperoleh melalui Kaggle. Dataset baru akan disesuaikan dengan dataset lama agar dapat dimerge dan dianalisis
"""

!kaggle datasets download -d rishitjavia/netflix-movie-rating-dataset

# ekstrak data
!unzip netflix-movie-rating-dataset.zip

"""Telah diekstrak 2 dataset dari Kaggle, yang selanjutnya akan dimerge untuk memudahkan evaluasi."""

# identify df and merge right.

movie_df = pd.read_csv('Netflix_Dataset_Movie.csv')
user_df = pd.read_csv('Netflix_Dataset_Rating.csv')

merged_df = pd.merge(movie_df, user_df, on='Movie_ID', how='right')
merged_df

"""mengubah nama kolom pada dataframe baru merge_df agar dapat dimerge dengan dataframe lama df."""

# rename column and put in to new df
df2 = merged_df[['Name', 'User_ID', 'Rating']].rename(columns={'Name': 'title', 'Rating': 'rating_user'})
df2.head()

"""tampak bahwa rating_user memiliki skala 1-5. Kita ubah formatnya menjadi 1-10 agar dapat sesuai dengan format rating pada df."""

df2['rating_user'] = df2['rating_user'] * 2

"""membuat dataframe baru yang menggabungkan dataframe awal df dengan dataset baru df2"""

# Ambil hanya kolom 'title' dari df (metadata film)
df_metadata = df[['title']].drop_duplicates()

# Merge hanya film yang ada di df_metadata (df)
merged_df_final = pd.merge(df2, df_metadata, on='title', how='inner')

merged_df_final.head()

# count unique tittle in merged_df_final
print(merged_df_final['title'].nunique())

"""terdapat 64 film dari data final

# Evaluation

Pada tahap evaluasi, dilakukan evaluasi terhadap kinerja kedua model yang dibuat dan akan dikomparasikan kinerjanya satu sama lain.

## Content-Based Recommender

Pada tahap ini dilakukan evaluasi terhadap kinerja model content-based recommender (menggunakan 1 film yang disukai user sebagai seed untuk rekomendasi). Teknik evaluasi yang digunakan yaitu dengan average precision dan average recall.
"""

def evaluate_content_based_recommender(merged_df_final, get_recommendations_fn, top_n=10, n_users=50):
    users = merged_df_final['User_ID'].unique()[:n_users]
    precision_list, recall_list = [], []

    for user in users:
        user_data = merged_df_final[merged_df_final['User_ID'] == user]
        liked_movies = user_data[user_data['rating_user'] >= 8]['title'].str.lower().tolist()

        if not liked_movies:
            continue

        seed_title = liked_movies[0]  # gunakan satu film sebagai seed
        try:
            recs = get_recommendations_fn(seed_title)
            if isinstance(recs, str):
                continue
            recommended = set(recs['title'].str.lower().tolist()[:top_n])
        except:
            continue

        relevant = set(liked_movies)
        true_positive = recommended & relevant

        precision = len(true_positive) / len(recommended) if recommended else 0
        recall = len(true_positive) / len(relevant) if relevant else 0

        precision_list.append(precision)
        recall_list.append(recall)

    avg_precision = sum(precision_list) / len(precision_list) if precision_list else 0
    avg_recall = sum(recall_list) / len(recall_list) if recall_list else 0

    print(f"Content-Based Evaluation")
    print(f"Average Precision@{top_n}: {avg_precision:.4f}")
    print(f"Average Recall@{top_n}: {avg_recall:.4f}")

evaluate_content_based_recommender(merged_df_final, get_recommendations, top_n=10)

"""## Hybrid recommender

Pada tahap ini akan dilakukan evaluasi terhadap model Hybrid recommender. Teknik evaluasi yang digunakan yaitu dengan average precision dan average recall.
"""

def evaluate_hybrid_recommender(merged_df_final, get_hybrid_recommendations_fn, top_n=10, n_users=50):
    users = merged_df_final['User_ID'].unique()[:n_users]
    precision_list, recall_list = [], []

    for user in users:
        user_data = merged_df_final[merged_df_final['User_ID'] == user]
        liked_movies = user_data[user_data['rating_user'] >= 8]['title'].str.lower().tolist()

        if not liked_movies:
            continue

        recommended = set()
        for title in liked_movies:
            try:
                recs = get_hybrid_recommendations_fn(title)
                if isinstance(recs, str):
                    continue
                recs = recs['title'].str.lower().tolist()
                recommended.update(recs[:top_n])
            except:
                continue

        relevant = set(liked_movies)
        true_positive = relevant & recommended

        precision = len(true_positive) / len(recommended) if recommended else 0
        recall = len(true_positive) / len(relevant) if relevant else 0

        precision_list.append(precision)
        recall_list.append(recall)

    avg_precision = sum(precision_list) / len(precision_list) if precision_list else 0
    avg_recall = sum(recall_list) / len(recall_list) if recall_list else 0

    print(f"Average Precision@{top_n}: {avg_precision:.4f}")
    print(f"Average Recall@{top_n}: {avg_recall:.4f}")

evaluate_hybrid_recommender(merged_df_final=merged_df_final, get_hybrid_recommendations_fn=get_hybrid_recommendations, top_n=10, n_users=100)

"""Di atas adalah hasil evaluasi model yang dilakukan. Model memiliki skor evaluasi yang rendah, dainggap penyebabnya karena data yang digunakan sedikit  (64 data film final) sehingga akurasinya kurang."""